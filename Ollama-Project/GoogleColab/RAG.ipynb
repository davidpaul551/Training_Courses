{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests faiss-cpu sentence-transformers python-dotenv"
      ],
      "metadata": {
        "id": "ZdRKz_xRFMB1",
        "outputId": "37db830f-2435-41ba-a59d-3b75678c2651",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.49.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0 python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import requests\n",
        "import faiss\n",
        "from google.colab import userdata\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "7JGPSwmEINC6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found! Set it using os.environ or a .env file.\")\n",
        "endpoint = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}"
      ],
      "metadata": {
        "id": "Hh1ot9rOIRo0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "VFn8FTrnQKyB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents():\n",
        "    \"\"\"Load predefined documents for retrieval.\"\"\"\n",
        "    return [\n",
        "        \"Fast language models improve efficiency in NLP applications by minimizing latency and enhancing user experience.\",\n",
        "        \"Machine learning models require high computational power to generate responses quickly in real-time applications.\",\n",
        "        \"Retrieval-augmented generation improves response accuracy by combining document retrieval with language models.\",\n",
        "        \"FAISS is an efficient library for fast nearest neighbor search, used for document retrieval.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "H_xaea0nQH9y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_faiss_index(documents):\n",
        "    \"\"\"Create a FAISS index from document embeddings.\"\"\"\n",
        "    document_embeddings = embedding_model.encode(documents, convert_to_tensor=True).cpu().detach().numpy()\n",
        "    dimension = document_embeddings.shape[1]\n",
        "\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(document_embeddings)\n",
        "\n",
        "    return index, documents"
      ],
      "metadata": {
        "id": "vdeMtFbHQPTb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_documents(query, index, documents, k=2):\n",
        "    \"\"\"Retrieve top k similar documents based on the query.\"\"\"\n",
        "    query_embedding = embedding_model.encode([query], convert_to_tensor=True).cpu().detach().numpy()\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    retrieved_docs = [documents[i] for i in indices[0]]\n",
        "    return \" \".join(retrieved_docs)"
      ],
      "metadata": {
        "id": "p6XWSllSQVDy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_response(context, query):\n",
        "    \"\"\"Send the query with retrieved context to the Groq API and return the response.\"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.3-70b-versatile\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Using the following context: '{context}', {query}\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(endpoint, headers=headers, json=payload, verify=False)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return f\"Error {response.status_code}: {response.text}\""
      ],
      "metadata": {
        "id": "Hoa1jYvJQbnI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to execute retrieval-augmented generation (RAG) flow.\"\"\"\n",
        "    documents = load_documents()\n",
        "    print(f\"Loaded {len(documents)} predefined documents.\")\n",
        "\n",
        "    index, documents = create_faiss_index(documents)\n",
        "\n",
        "    query = \"Explain the importance of fast language models\"\n",
        "    context = retrieve_documents(query, index, documents)\n",
        "    print(f\"Retrieved Context: {context}\")\n",
        "\n",
        "    response = generate_response(context, query)\n",
        "    print(\"Generated Response:\", response)\n"
      ],
      "metadata": {
        "id": "UX9P1u4iQkG_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "BufwhzJCSV1k",
        "outputId": "4c2b4e90-d88c-4feb-bd00-969815835877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4 predefined documents.\n",
            "Retrieved Context: Fast language models improve efficiency in NLP applications by minimizing latency and enhancing user experience. Retrieval-augmented generation improves response accuracy by combining document retrieval with language models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.groq.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: Fast language models play a vital role in Natural Language Processing (NLP) applications by significantly enhancing efficiency, user experience, and overall performance. The importance of fast language models can be understood from the following aspects:\n",
            "\n",
            "1. **Minimizing Latency**: Fast language models process and respond to user queries in real-time, minimizing latency and wait times. This is particularly crucial in applications where timely responses are essential, such as customer service chatbots, voice assistants, and real-time language translation.\n",
            "\n",
            "2. **Enhancing User Experience**: By providing quick and accurate responses, fast language models improve the overall user experience. Users are more likely to engage with an application or system that responds promptly, making it more likely to achieve its intended purpose, whether that's providing information, answering questions, or completing tasks.\n",
            "\n",
            "3. **Improving Response Accuracy**: When combined with retrieval-augmented generation, fast language models can leverage document retrieval to provide more accurate and informative responses. This approach enables the model to retrieve relevant documents and incorporate the information into its response, leading to more accurate and contextually relevant outputs.\n",
            "\n",
            "4. **Efficient Resource Utilization**: Fast language models are designed to optimize computational resources, reducing the need for extensive processing power and memory. This makes them more efficient and cost-effective, allowing developers to deploy NLP applications on a wider range of devices, from smartphones to servers.\n",
            "\n",
            "5. **Scalability and Reliability**: Fast language models enable NLP applications to handle a large volume of user requests concurrently, making them more scalable and reliable. This is particularly important in applications where a high volume of users is expected, such as language translation services or chatbots.\n",
            "\n",
            "In summary, the importance of fast language models lies in their ability to provide efficient, accurate, and timely responses, leading to enhanced user experience, improved resource utilization, and increased scalability. By minimizing latency and combining with retrieval-augmented generation, fast language models have become a crucial component of modern NLP applications.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}