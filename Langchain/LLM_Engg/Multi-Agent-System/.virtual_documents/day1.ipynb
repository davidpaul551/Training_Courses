


pip install modal


# Just one import to start with!!

import modal





# Remove the '# ' from the next line and run the cell
!modal setup


!modal token new


!modal token set --token-id ak-jQHflODSwGNvab97cT9lRY --token-secret as-DKP5ZJY5GM3AG1LdZm6x32


!curl -I https://api.modal.com


!ping api.modal.com


!modal --version


import os
os.environ["MODAL_TOKEN_ID"] = "ak-jQHflODSwGNvab97cT9lRY"
os.environ["MODAL_TOKEN_SECRET"] = "as-DKP5ZJY5GM3AG1LdZm6x32"


!modal profile list


cat /root/.modal.toml


%cd /content/drive/MyDrive/Multi-Agent-System
!ls -lah  # Check if hello.py is there



import sys
sys.path.append('/content/drive/MyDrive/Multi-Agent-System')


from hello import app, hello


from google.colab import drive
drive.mount('/content/drive')


with app.run():
    reply=hello.local()
print(reply)


# Cell 1: Install, set tokens, and define files

# Write modal_tokens.py (for local token setting)
with open("modal_tokens.py", "w") as f:
    f.write("""
def load_tokens():
    import os
    os.environ["MODAL_TOKEN_ID"] = "ak-jQHflODSwGNvab97cT9lRY"
    os.environ["MODAL_TOKEN_SECRET"] = "as-DKP5ZJY5GM3AG1LdZm6x32"
""")

# Write hello.py (no modal_tokens import)
with open("hello.py", "w") as f:
    f.write("""
import modal
from modal import App, Image

app = modal.App("hello-v2")  # Unique name to avoid old deployment
image = Image.debian_slim().pip_install("requests")

@app.function(image=image)
def hello() -> str:
    import requests
    response = requests.get('https://ipinfo.io/json')
    data = response.json()
    city, region, country = data['city'], data['region'], data['country']
    return f"Hello from {city}, {region}, {country}!!"
""")

# Set tokens locally and import app
from modal_tokens import load_tokens
load_tokens()
from hello import app, hello

# Deploy the app (optional explicit deployment)
with app.run():
    print("App deployed successfully")


# Cell 2: Run remotely
from hello import app, hello

with app.run():
    reply = hello.remote()
    print(reply)





# Install modal

# Set Modal tokens locally
with open("modal_tokens.py", "w") as f:
    f.write("""
def load_tokens():
    import os
    os.environ["MODAL_TOKEN_ID"] = "ak-jQHflODSwGNvab97cT9lRY"
    os.environ["MODAL_TOKEN_SECRET"] = "as-DKP5ZJY5GM3AG1LdZm6x32"
""")

# Define the app in llama.py with huggingface-secret and a new app name
with open("llama.py", "w") as f:
    f.write("""
import modal
from modal import App, Volume, Image

app = modal.App("llama-v3")  # New unique name
image = Image.debian_slim().pip_install("torch", "transformers", "bitsandbytes", "accelerate")
secrets = [modal.Secret.from_name("huggingface-secret")]
GPU = "T4"
MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

@app.function(image=image, secrets=secrets, gpu=GPU, timeout=1800)
def generate(prompt: str) -> str:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed

    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4"
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=quant_config,
        device_map="auto"
    )

    set_seed(42)
    inputs = tokenizer.encode(prompt, return_tensors="pt").to("cuda")
    attention_mask = torch.ones(inputs.shape, device="cuda")
    outputs = model.generate(inputs, attention_mask=attention_mask, max_new_tokens=5, num_return_sequences=1)
    return tokenizer.decode(outputs[0])
""")

# Load tokens and import app
from modal_tokens import load_tokens
load_tokens()

# Clear any old module cache
import sys
if "llama" in sys.modules:
    del sys.modules["llama"]

from llama import app, generate

# Verify the file contents
print("Contents of llama.py:")
!cat llama.py


# Run remotely with output
from llama import app, generate

with modal.enable_output():  # Show Modal logs
    with app.run():
        result = generate.remote("Life is a mystery, everyone must stand alone, I hear")
        print(result)


import modal
from pricer_ephemeral import app, price


# Set Modal tokens locally
with open("modal_tokens.py", "w") as f:
    f.write("""
def load_tokens():
    import os
    os.environ["MODAL_TOKEN_ID"] = "ak-jQHflODSwGNvab97cT9lRY"
    os.environ["MODAL_TOKEN_SECRET"] = "as-DKP5ZJY5GM3AG1LdZm6x32"
""")

# Define the app in llama.py with huggingface-secret and a new app name
with open("price_ephemeral.py", "w") as f:
    f.write("""import modal
from modal import App, Image

# Setup

app = modal.App("pricer")
image = Image.debian_slim().pip_install("torch", "transformers", "bitsandbytes", "accelerate", "peft")
secrets = [modal.Secret.from_name("huggingface-secret")]

# Constants

GPU = "T4"
BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
PROJECT_NAME = "pricer"
HF_USER = "ed-donner" # your HF name here! Or use mine if you just want to reproduce my results.
RUN_NAME = "2024-09-13_13.04.39"
PROJECT_RUN_NAME = f"{PROJECT_NAME}-{RUN_NAME}"
REVISION = "e8d637df551603dc86cd7a1598a8f44af4d7ae36"
FINETUNED_MODEL = f"{HF_USER}/{PROJECT_RUN_NAME}"


@app.function(image=image, secrets=secrets, gpu=GPU, timeout=1800)
def price(description: str) -> float:
    import os
    import re
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed
    from peft import PeftModel

    QUESTION = "How much does this cost to the nearest dollar?"
    PREFIX = "Price is $"

    prompt = f"{QUESTION}\n{description}\n{PREFIX}"

    # Quant Config
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4"
    )

    # Load model and tokenizer

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quant_config,
        device_map="auto"
    )


    set_seed(42)
    inputs = tokenizer.encode(prompt, return_tensors="pt").to("cuda")
    attention_mask = torch.ones(inputs.shape, device="cuda")
    outputs = base_model.generate(inputs, attention_mask=attention_mask, max_new_tokens=5, num_return_sequences=1)
    result = tokenizer.decode(outputs[0])

    contents = result.split("Price is $")[1]
    contents = contents.replace(',','')
    match = re.search(r"[-+]?\d*\.\d+|\d+", contents)
    return float(match.group()) if match else 0

""")

# Load tokens and import app
from modal_tokens import load_tokens
load_tokens()

# Clear any old module cache
import sys
if "pricer_ephemeral" in sys.modules:
    del sys.modules["pricer_ephemeral"]

from pricer_ephemeral import app, price

# Verify the file contents
print("Contents of prices.py:")
!cat pricer_ephemeral.py


with modal.enable_output():
    with app.run():
        result=price.remote("Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio")
        print(result)





!modal deploy pricer_service


pricer = modal.Function.lookup("pricer-service", "price")


pricer.remote("Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio")


# You can also run "modal deploy pricer_service2" at the command line in an activated environment
!modal deploy pricer_service2


Pricer = modal.Cls.lookup("pricer-service", "Pricer")
pricer = Pricer()
reply = pricer.price.remote("Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio")
print(reply)








from agents.specialist_agent import SpecialistAgent


agent = SpecialistAgent()
agent.price("iPad Pro 2nd generation")



